{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Hackathon Sentiment Analysis Improved\n",
                "\n",
                "This notebook integrates Play Store and YouTube data with the main dataset (`CoreTax Combined Data Clean.csv`), improves sentiment labeling using RoBERTa, and includes detailed analysis (Temporal, Source-based, Content-based)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install -q transformers torch pandas numpy scikit-learn matplotlib seaborn wordcloud networkx\n",
                "!pip install -q indobenchmark-toolkit\n",
                "!pip install -q Sastrawi\n",
                "!pip install -q accelerate -U"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from wordcloud import WordCloud\n",
                "import re\n",
                "import warnings\n",
                "import requests\n",
                "from io import StringIO\n",
                "import os\n",
                "import networkx as nx\n",
                "from collections import Counter\n",
                "from itertools import combinations\n",
                "\n",
                "# Import untuk preprocessing\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "\n",
                "# Import untuk IndoBERT\n",
                "import torch\n",
                "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, pipeline\n",
                "from torch.utils.data import Dataset\n",
                "\n",
                "# Import Sastrawi untuk stemming\n",
                "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
                "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
                "\n",
                "# Setup plotting style\n",
                "sns.set(style=\"whitegrid\")\n",
                "warnings.filterwarnings('ignore')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Mount GDrive dan Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    from google.colab import drive\n",
                "    print(\"Mounting Google Drive...\")\n",
                "    drive.mount('/content/drive/')\n",
                "    print(\"✓ Google Drive mounted!\")\n",
                "    # Set data path for Colab\n",
                "    DATA_PATH = '/content/drive/MyDrive/Hackathon/data/'\n",
                "    DATA_ACTUAL_PATH = '/content/drive/MyDrive/Hackathon/data/processed/'\n",
                "    MODELS_PATH = '/content/drive/MyDrive/Hackathon/models/'\n",
                "    OUTPUTS_PATH = '/content/drive/MyDrive/Hackathon/outputs/'\n",
                "except ImportError:\n",
                "    print(\"⚠️ Google Colab environment not detected. Using local paths.\")\n",
                "    DATA_PATH = '../data/'\n",
                "    DATA_ACTUAL_PATH = '../data/processed/'\n",
                "    MODELS_PATH = '../models/'\n",
                "    OUTPUTS_PATH = '../outputs/'\n",
                "\n",
                "# Ensure output directories exist\n",
                "os.makedirs(MODELS_PATH, exist_ok=True)\n",
                "os.makedirs(OUTPUTS_PATH, exist_ok=True)\n",
                "\n",
                "print(f\"Data path set to: {DATA_PATH}\")\n",
                "print(f\"Models path set to: {MODELS_PATH}\")\n",
                "print(f\"Outputs path set to: {OUTPUTS_PATH}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Load RoBERTa Sentiment Classifier"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load RoBERTa sentiment classifier\n",
                "print(\"Loading RoBERTa sentiment classifier...\")\n",
                "try:\n",
                "    sentiment_classifier = pipeline(\n",
                "        \"text-classification\",\n",
                "        model=\"w11wo/indonesian-roberta-base-sentiment-classifier\"\n",
                "    )\n",
                "    print(\"✓ RoBERTa sentiment classifier loaded!\")\n",
                "except Exception as e:\n",
                "    print(f\"⚠️ Error loading RoBERTa classifier: {e}\")\n",
                "    sentiment_classifier = None\n",
                "\n",
                "def roberta_sentiment_labeling(text, classifier):\n",
                "    \"\"\"\n",
                "    Labeling sentimen menggunakan RoBERTa classifier\n",
                "    \"\"\"\n",
                "    if not isinstance(text, str) or len(text.strip()) == 0:\n",
                "        return 'neutral', 0.0\n",
                "\n",
                "    if classifier is None:\n",
                "        return 'neutral', 0.0\n",
                "\n",
                "    try:\n",
                "        # Predict sentiment\n",
                "        result = classifier(text[:512])[0]  # Limit to 512 chars for efficiency\n",
                "\n",
                "        # Map label to sentiment\n",
                "        label = result['label'].lower()\n",
                "        score = result['score']\n",
                "\n",
                "        # Model outputs: positive, negative, neutral\n",
                "        if 'positive' in label:\n",
                "            return 'positive', score\n",
                "        elif 'negative' in label:\n",
                "            return 'negative', score\n",
                "        else:\n",
                "            return 'neutral', score\n",
                "\n",
                "    except Exception as e:\n",
                "        # print(f\"Error in RoBERTa labeling: {e}\")\n",
                "        return 'neutral', 0.0\n",
                "\n",
                "print(\"✓ RoBERTa labeling function ready!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Data Loading and Integration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Datasets\n",
                "print(\"Loading datasets...\")\n",
                "try:\n",
                "    df_youtube = pd.read_csv(os.path.join(DATA_PATH, 'Scraper Youtube CoreTax.csv'))\n",
                "except FileNotFoundError:\n",
                "    df_youtube = pd.read_csv('../data/Scraper Youtube CoreTax.csv')\n",
                "\n",
                "try:\n",
                "    df_playstore = pd.read_csv(os.path.join(DATA_PATH, 'CoreTax Scraper M Pajak 2025.csv'))\n",
                "except FileNotFoundError:\n",
                "    df_playstore = pd.read_csv('../data/CoreTax Scraper M Pajak 2025.csv')\n",
                "\n",
                "try:\n",
                "    df_combined_old = pd.read_csv(os.path.join(DATA_ACTUAL_PATH, 'CoreTax Combined Data Clean.csv'))\n",
                "except FileNotFoundError:\n",
                "    df_combined_old = pd.read_csv('../data/processed/CoreTax Combined Data Clean.csv')\n",
                "\n",
                "print(f\"YouTube Data: {len(df_youtube)} rows\")\n",
                "print(f\"Play Store Data: {len(df_playstore)} rows\")\n",
                "print(f\"Existing Combined Data: {len(df_combined_old)} rows\")\n",
                "\n",
                "# Standardize Columns\n",
                "# Youtube: date, text, source\n",
                "df_youtube = df_youtube[['date', 'text', 'source']].copy()\n",
                "df_youtube['rating'] = np.nan\n",
                "\n",
                "# Play Store: rating, at -> date, content -> text, source\n",
                "df_playstore = df_playstore.rename(columns={'at': 'date', 'content': 'text'})\n",
                "df_playstore = df_playstore[['date', 'text', 'source', 'rating']].copy()\n",
                "\n",
                "# Combined Old: date, text, source, rating\n",
                "# Ensure columns exist\n",
                "if 'date' not in df_combined_old.columns:\n",
                "    df_combined_old['date'] = np.nan\n",
                "if 'rating' not in df_combined_old.columns:\n",
                "    df_combined_old['rating'] = np.nan\n",
                "\n",
                "df_combined_old = df_combined_old[['date', 'text', 'source', 'rating']].copy()\n",
                "\n",
                "# Combine all data\n",
                "df_combined = pd.concat([df_youtube, df_playstore, df_combined_old], ignore_index=True)\n",
                "\n",
                "# Drop duplicates based on text to avoid repetition\n",
                "df_combined = df_combined.drop_duplicates(subset=['text'])\n",
                "\n",
                "# Convert date to datetime\n",
                "df_combined['date'] = pd.to_datetime(df_combined['date'], errors='coerce')\n",
                "\n",
                "print(f\"Total Combined Data after deduplication: {len(df_combined)}\")\n",
                "print(\"Source Distribution:\")\n",
                "print(df_combined['source'].value_counts())\n",
                "df_combined.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Data Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def clean_text(text):\n",
                "    if not isinstance(text, str):\n",
                "        return \"\"\n",
                "    # Remove URLs\n",
                "    text = re.sub(r'http\\S+', '', text)\n",
                "    # Remove Mentions\n",
                "    text = re.sub(r'@\\w+', '', text)\n",
                "    # Remove special characters but keep punctuation that might be useful for RoBERTa (though usually we clean it for WordCloud)\n",
                "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
                "    text = text.lower()\n",
                "    text = re.sub(r'\\s+', ' ', text).strip()\n",
                "    return text\n",
                "\n",
                "# Visualization: WordCloud BEFORE Preprocessing\n",
                "print(\"Generating WordCloud BEFORE Preprocessing...\")\n",
                "all_text_before = ' '.join([str(t) for t in df_combined['text'].fillna('')])\n",
                "wordcloud_before = WordCloud(width=800, height=400, background_color='white').generate(all_text_before)\n",
                "\n",
                "plt.figure(figsize=(10, 5))\n",
                "plt.imshow(wordcloud_before, interpolation='bilinear')\n",
                "plt.axis('off')\n",
                "plt.title('WordCloud BEFORE Preprocessing')\n",
                "plt.savefig(os.path.join(OUTPUTS_PATH, 'wordcloud_before.png'))\n",
                "plt.show()\n",
                "\n",
                "# Apply Cleaning\n",
                "print(\"Cleaning text...\")\n",
                "df_combined['cleaned_text'] = df_combined['text'].apply(clean_text)\n",
                "\n",
                "# Visualization: WordCloud AFTER Preprocessing\n",
                "print(\"Generating WordCloud AFTER Preprocessing...\")\n",
                "all_text_after = ' '.join([str(t) for t in df_combined['cleaned_text'].fillna('')])\n",
                "wordcloud_after = WordCloud(width=800, height=400, background_color='white').generate(all_text_after)\n",
                "\n",
                "plt.figure(figsize=(10, 5))\n",
                "plt.imshow(wordcloud_after, interpolation='bilinear')\n",
                "plt.axis('off')\n",
                "plt.title('WordCloud AFTER Preprocessing')\n",
                "plt.savefig(os.path.join(OUTPUTS_PATH, 'wordcloud_after.png'))\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Sentiment Labeling with RoBERTa"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Apply to dataset (This might take a while)\n",
                "print(\"Labeling data with RoBERTa... This may take some time.\")\n",
                "\n",
                "# Using cleaned_text for consistency\n",
                "df_combined[['roberta_sentiment', 'roberta_score']] = df_combined['cleaned_text'].apply(\n",
                "    lambda x: pd.Series(roberta_sentiment_labeling(x, sentiment_classifier))\n",
                ")\n",
                "\n",
                "print(\"Labeling complete!\")\n",
                "print(df_combined['roberta_sentiment'].value_counts())\n",
                "df_combined.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Detailed Analysis & Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Sentiment Distribution (Bar Chart)\n",
                "plt.figure(figsize=(8, 6))\n",
                "sns.countplot(x='roberta_sentiment', data=df_combined, palette='viridis')\n",
                "plt.title('Overall Sentiment Distribution (RoBERTa)')\n",
                "plt.xlabel('Sentiment')\n",
                "plt.ylabel('Count')\n",
                "plt.savefig(os.path.join(OUTPUTS_PATH, 'sentiment_distribution_bar.png'))\n",
                "plt.show()\n",
                "\n",
                "# 2. Sentiment Distribution (Pie Chart)\n",
                "plt.figure(figsize=(8, 8))\n",
                "sentiment_counts = df_combined['roberta_sentiment'].value_counts()\n",
                "plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%', startangle=140, colors=sns.color_palette('viridis', 3))\n",
                "plt.title('Sentiment Proportion')\n",
                "plt.savefig(os.path.join(OUTPUTS_PATH, 'sentiment_distribution_pie.png'))\n",
                "plt.show()\n",
                "\n",
                "# 3. Sentiment by Source\n",
                "plt.figure(figsize=(12, 6))\n",
                "sns.countplot(x='source', hue='roberta_sentiment', data=df_combined, palette='viridis')\n",
                "plt.title('Sentiment Distribution by Source')\n",
                "plt.xlabel('Source')\n",
                "plt.ylabel('Count')\n",
                "plt.legend(title='Sentiment')\n",
                "plt.savefig(os.path.join(OUTPUTS_PATH, 'sentiment_by_source.png'))\n",
                "plt.show()\n",
                "\n",
                "# 4. Confidence Score Distribution (Box Plot)\n",
                "plt.figure(figsize=(10, 6))\n",
                "sns.boxplot(x='roberta_sentiment', y='roberta_score', data=df_combined, palette='viridis')\n",
                "plt.title('Distribution of Confidence Scores by Sentiment')\n",
                "plt.xlabel('Sentiment')\n",
                "plt.ylabel('Confidence Score')\n",
                "plt.savefig(os.path.join(OUTPUTS_PATH, 'confidence_score_distribution.png'))\n",
                "plt.show()\n",
                "\n",
                "# 5. Temporal Analysis (Sentiment over Time)\n",
                "# Filter out rows with missing dates\n",
                "df_time = df_combined.dropna(subset=['date']).copy()\n",
                "df_time['month_year'] = df_time['date'].dt.to_period('M')\n",
                "\n",
                "plt.figure(figsize=(14, 6))\n",
                "sentiment_time = df_time.groupby(['month_year', 'roberta_sentiment']).size().unstack(fill_value=0)\n",
                "sentiment_time.plot(kind='line', marker='o', figsize=(14, 6))\n",
                "plt.title('Sentiment Trends Over Time (Monthly)')\n",
                "plt.xlabel('Month-Year')\n",
                "plt.ylabel('Count')\n",
                "plt.grid(True)\n",
                "plt.savefig(os.path.join(OUTPUTS_PATH, 'sentiment_trends_time.png'))\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6. Word Clouds by Sentiment\n",
                "for sentiment in ['positive', 'negative', 'neutral']:\n",
                "    subset = df_combined[df_combined['roberta_sentiment'] == sentiment]\n",
                "    if not subset.empty:\n",
                "        text = ' '.join([str(t) for t in subset['cleaned_text'].fillna('')])\n",
                "        if len(text) > 0:\n",
                "            wc = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
                "            plt.figure(figsize=(10, 5))\n",
                "            plt.imshow(wc, interpolation='bilinear')\n",
                "            plt.axis('off')\n",
                "            plt.title(f'WordCloud - {sentiment.capitalize()} Sentiment')\n",
                "            plt.savefig(os.path.join(OUTPUTS_PATH, f'wordcloud_{sentiment}.png'))\n",
                "            plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Advanced N-gram Analysis (Bigram & Trigram)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_ngrams(text_series, n=2, top_k=20, title=\"N-grams\", filename=\"ngrams.png\"):\n",
                "    vec = CountVectorizer(ngram_range=(n, n), stop_words=None).fit(text_series)\n",
                "    bag_of_words = vec.transform(text_series)\n",
                "    sum_words = bag_of_words.sum(axis=0)\n",
                "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
                "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
                "    \n",
                "    common_words = words_freq[:top_k]\n",
                "    df_ngram = pd.DataFrame(common_words, columns=['ngram', 'count'])\n",
                "    \n",
                "    plt.figure(figsize=(12, 6))\n",
                "    sns.barplot(x='count', y='ngram', data=df_ngram, palette='viridis')\n",
                "    plt.title(title)\n",
                "    plt.savefig(os.path.join(OUTPUTS_PATH, filename))\n",
                "    plt.show()\n",
                "\n",
                "# Plot Bigrams (AFTER Preprocessing)\n",
                "plot_ngrams(df_combined['cleaned_text'], n=2, title=\"Top 20 Bigrams (After Preprocessing)\", filename=\"bigrams.png\")\n",
                "\n",
                "# Plot Trigrams (AFTER Preprocessing)\n",
                "plot_ngrams(df_combined['cleaned_text'], n=3, title=\"Top 20 Trigrams (After Preprocessing)\", filename=\"trigrams.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Co-occurrence Network Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_cooccurrence_network(text_series, top_n=30):\n",
                "    \"\"\"\n",
                "    Plots a co-occurrence network of the most frequent words.\n",
                "    \"\"\"\n",
                "    # Tokenize and count words\n",
                "    all_words = ' '.join(text_series).split()\n",
                "    word_counts = Counter(all_words)\n",
                "    top_words = [word for word, count in word_counts.most_common(top_n)]\n",
                "    \n",
                "    # Build co-occurrence matrix\n",
                "    co_occurrence = Counter()\n",
                "    for text in text_series:\n",
                "        words = set(text.split())\n",
                "        # Only consider top words\n",
                "        words = [w for w in words if w in top_words]\n",
                "        for w1, w2 in combinations(sorted(words), 2):\n",
                "            co_occurrence[(w1, w2)] += 1\n",
                "            \n",
                "    # Create Graph\n",
                "    G = nx.Graph()\n",
                "    for (w1, w2), count in co_occurrence.items():\n",
                "        if count > 5: # Threshold to reduce clutter\n",
                "            G.add_edge(w1, w2, weight=count)\n",
                "            \n",
                "    plt.figure(figsize=(12, 12))\n",
                "    pos = nx.spring_layout(G, k=0.5)\n",
                "    \n",
                "    # Draw nodes and edges\n",
                "    nx.draw_networkx_nodes(G, pos, node_size=1000, node_color='skyblue', alpha=0.7)\n",
                "    nx.draw_networkx_edges(G, pos, width=[G[u][v]['weight']/5 for u,v in G.edges()], alpha=0.5)\n",
                "    nx.draw_networkx_labels(G, pos, font_size=10, font_family='sans-serif')\n",
                "    \n",
                "    plt.title('Word Co-occurrence Network (Top Words)')\n",
                "    plt.axis('off')\n",
                "    plt.savefig(os.path.join(OUTPUTS_PATH, 'cooccurrence_network.png'))\n",
                "    plt.show()\n",
                "\n",
                "print(\"Generating Co-occurrence Network...\")\n",
                "plot_cooccurrence_network(df_combined['cleaned_text'])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Advanced TF-IDF Analysis (Per Sentiment)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_top_tfidf_words(df, sentiment_col, text_col, sentiment_label, top_n=15):\n",
                "    \"\"\"\n",
                "    Plots the top TF-IDF words for a specific sentiment.\n",
                "    \"\"\"\n",
                "    subset = df[df[sentiment_col] == sentiment_label]\n",
                "    if subset.empty:\n",
                "        print(f\"No data for sentiment: {sentiment_label}\")\n",
                "        return\n",
                "\n",
                "    tfidf = TfidfVectorizer(max_features=1000, stop_words=None) # You can add Indonesian stopwords here\n",
                "    try:\n",
                "        tfidf_matrix = tfidf.fit_transform(subset[text_col])\n",
                "    except ValueError:\n",
                "        print(f\"Not enough data to perform TF-IDF for {sentiment_label}\")\n",
                "        return\n",
                "\n",
                "    feature_names = tfidf.get_feature_names_out()\n",
                "    dense = tfidf_matrix.todense()\n",
                "    denselist = dense.tolist()\n",
                "    \n",
                "    # Sum TF-IDF scores for each word\n",
                "    df_tfidf = pd.DataFrame(denselist, columns=feature_names)\n",
                "    top_words = df_tfidf.sum().sort_values(ascending=False).head(top_n)\n",
                "    \n",
                "    plt.figure(figsize=(10, 6))\n",
                "    sns.barplot(x=top_words.values, y=top_words.index, palette='magma')\n",
                "    plt.title(f'Top {top_n} TF-IDF Words for {sentiment_label.capitalize()} Sentiment')\n",
                "    plt.xlabel('TF-IDF Score Sum')\n",
                "    plt.savefig(os.path.join(OUTPUTS_PATH, f'tfidf_{sentiment_label}.png'))\n",
                "    plt.show()\n",
                "\n",
                "print(\"Generating TF-IDF Plots per Sentiment...\")\n",
                "for sentiment in ['positive', 'negative', 'neutral']:\n",
                "    plot_top_tfidf_words(df_combined, 'roberta_sentiment', 'cleaned_text', sentiment)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Recommendation Analysis (Automated Insights)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract top negative bigrams to identify pain points\n",
                "negative_text = df_combined[df_combined['roberta_sentiment'] == 'negative']['cleaned_text']\n",
                "\n",
                "if not negative_text.empty:\n",
                "    vec = CountVectorizer(ngram_range=(2, 2)).fit(negative_text)\n",
                "    bag_of_words = vec.transform(negative_text)\n",
                "    sum_words = bag_of_words.sum(axis=0)\n",
                "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
                "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
                "    \n",
                "    top_negative_bigrams = words_freq[:10]\n",
                "    \n",
                "    # Create a recommendation table based on common issues\n",
                "    recommendations = []\n",
                "    for bigram, count in top_negative_bigrams:\n",
                "        issue = bigram\n",
                "        rec = \"Investigate specific error logs and user reports related to this term.\"\n",
                "        \n",
                "        if \"login\" in bigram or \"masuk\" in bigram:\n",
                "            rec = \"Improve login stability and error messaging.\"\n",
                "        elif \"kode\" in bigram or \"verifikasi\" in bigram or \"otp\" in bigram:\n",
                "            rec = \"Check SMS/Email gateway latency and OTP delivery rates.\"\n",
                "        elif \"error\" in bigram or \"gagal\" in bigram:\n",
                "            rec = \"Conduct technical audit on server stability and API endpoints.\"\n",
                "        elif \"lambat\" in bigram or \"lemot\" in bigram:\n",
                "            rec = \"Optimize application performance and server response times.\"\n",
                "            \n",
                "        recommendations.append({'Issue (Bigram)': issue, 'Frequency': count, 'Recommendation': rec})\n",
                "    \n",
                "    df_rec = pd.DataFrame(recommendations)\n",
                "    \n",
                "    print(\"Automated Recommendations based on Negative Sentiment Analysis:\")\n",
                "    display(df_rec.style.background_gradient(cmap='Reds', subset=['Frequency']))\n",
                "    \n",
                "    # Save recommendations\n",
                "    df_rec.to_csv(os.path.join(OUTPUTS_PATH, 'recommendations.csv'), index=False)\n",
                "else:\n",
                "    print(\"No negative sentiment data found to generate recommendations.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# IndoBERT Fine-tuning & Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare Data for IndoBERT Fine-tuning\n",
                "# We will use the 'roberta_sentiment' labels as our target for this demonstration.\n",
                "# In a real scenario, you would use human-annotated labels.\n",
                "\n",
                "print(\"Preparing data for IndoBERT fine-tuning...\")\n",
                "\n",
                "# Encode labels\n",
                "le = LabelEncoder()\n",
                "df_combined['label_encoded'] = le.fit_transform(df_combined['roberta_sentiment'])\n",
                "label_map = {index: label for index, label in enumerate(le.classes_)}\n",
                "print(\"Label Mapping:\", label_map)\n",
                "\n",
                "# Split Data\n",
                "X_train, X_val, y_train, y_val = train_test_split(\n",
                "    df_combined['cleaned_text'], \n",
                "    df_combined['label_encoded'], \n",
                "    test_size=0.2, \n",
                "    random_state=42\n",
                ")\n",
                "\n",
                "# Load Tokenizer\n",
                "model_name = \"indobenchmark/indobert-base-p1\"\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "\n",
                "# Create Dataset Class\n",
                "class SentimentDataset(Dataset):\n",
                "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
                "        self.texts = texts\n",
                "        self.labels = labels\n",
                "        self.tokenizer = tokenizer\n",
                "        self.max_len = max_len\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.texts)\n",
                "\n",
                "    def __getitem__(self, item):\n",
                "        text = str(self.texts[item])\n",
                "        label = self.labels[item]\n",
                "\n",
                "        encoding = self.tokenizer.encode_plus(\n",
                "            text,\n",
                "            add_special_tokens=True,\n",
                "            max_length=self.max_len,\n",
                "            return_token_type_ids=False,\n",
                "            padding='max_length',\n",
                "            truncation=True,\n",
                "            return_attention_mask=True,\n",
                "            return_tensors='pt',\n",
                "        )\n",
                "\n",
                "        return {\n",
                "            'input_ids': encoding['input_ids'].flatten(),\n",
                "            'attention_mask': encoding['attention_mask'].flatten(),\n",
                "            'labels': torch.tensor(label, dtype=torch.long)\n",
                "        }\n",
                "\n",
                "# Create Datasets\n",
                "train_dataset = SentimentDataset(X_train.to_numpy(), y_train.to_numpy(), tokenizer)\n",
                "val_dataset = SentimentDataset(X_val.to_numpy(), y_val.to_numpy(), tokenizer)\n",
                "\n",
                "# Load Model\n",
                "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_map))\n",
                "\n",
                "# Define Metrics\n",
                "def compute_metrics(pred):\n",
                "    labels = pred.label_ids\n",
                "    preds = pred.predictions.argmax(-1)\n",
                "    acc = accuracy_score(labels, preds)\n",
                "    f1 = f1_score(labels, preds, average='weighted')\n",
                "    return {'accuracy': acc, 'f1': f1}\n",
                "\n",
                "# Training Arguments\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=os.path.join(MODELS_PATH, 'indobert_finetuned'),\n",
                "    num_train_epochs=1,              # Low epoch for demo purposes\n",
                "    per_device_train_batch_size=8,   # Adjust based on GPU memory\n",
                "    per_device_eval_batch_size=16,\n",
                "    warmup_steps=100,\n",
                "    weight_decay=0.01,\n",
                "    logging_dir='./logs',\n",
                "    logging_steps=10,\n",
                "    evaluation_strategy=\"steps\",\n",
                "    eval_steps=50,\n",
                "    save_steps=100,\n",
                "    load_best_model_at_end=True,\n",
                "    report_to=\"none\" # Disable wandb/mlflow logging\n",
                ")\n",
                "\n",
                "# Initialize Trainer\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=train_dataset,\n",
                "    eval_dataset=val_dataset,\n",
                "    compute_metrics=compute_metrics,\n",
                ")\n",
                "\n",
                "# Train Model\n",
                "print(\"Starting IndoBERT Fine-tuning...\")\n",
                "trainer.train()\n",
                "\n",
                "# Evaluate\n",
                "print(\"Evaluating Model...\")\n",
                "eval_result = trainer.evaluate()\n",
                "print(f\"Evaluation Results: {eval_result}\")\n",
                "\n",
                "# Save the final model\n",
                "trainer.save_model(os.path.join(MODELS_PATH, 'indobert_finetuned_final'))\n",
                "tokenizer.save_pretrained(os.path.join(MODELS_PATH, 'indobert_finetuned_final'))\n",
                "print(f\"Model saved to {os.path.join(MODELS_PATH, 'indobert_finetuned_final')}\")\n",
                "\n",
                "# Visualization: Confusion Matrix for IndoBERT\n",
                "print(\"Generating Confusion Matrix for IndoBERT...\")\n",
                "predictions = trainer.predict(val_dataset)\n",
                "preds = np.argmax(predictions.predictions, axis=-1)\n",
                "cm = confusion_matrix(predictions.label_ids, preds)\n",
                "\n",
                "plt.figure(figsize=(8, 6))\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)\n",
                "plt.title('Confusion Matrix (IndoBERT)')\n",
                "plt.xlabel('Predicted')\n",
                "plt.ylabel('Actual')\n",
                "plt.savefig(os.path.join(OUTPUTS_PATH, 'confusion_matrix_indobert.png'))\n",
                "plt.show()\n",
                "\n",
                "# Visualization: Training History (Loss)\n",
                "history = trainer.state.log_history\n",
                "loss_history = [x['loss'] for x in history if 'loss' in x]\n",
                "steps = [x['step'] for x in history if 'loss' in x]\n",
                "\n",
                "if loss_history:\n",
                "    plt.figure(figsize=(10, 5))\n",
                "    plt.plot(steps, loss_history, label='Training Loss')\n",
                "    plt.title('IndoBERT Training Loss')\n",
                "    plt.xlabel('Step')\n",
                "    plt.ylabel('Loss')\n",
                "    plt.legend()\n",
                "    plt.savefig(os.path.join(OUTPUTS_PATH, 'training_loss_indobert.png'))\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Save Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "output_path = '../data/processed/CoreTax_Combined_RoBERTa_Labeled.csv'\n",
                "df_combined.to_csv(output_path, index=False)\n",
                "print(f\"Processed data saved to {output_path}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}